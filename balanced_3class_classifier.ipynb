{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4f8e75d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da32c68",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a05d4543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (1500, 46)\n",
      "Test data shape: (377, 45)\n",
      "\n",
      "Class distribution BEFORE balancing:\n",
      "drug_category\n",
      "Hallucinogens    691\n",
      "Stimulants       567\n",
      "Depressants      242\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentages:\n",
      "drug_category\n",
      "Hallucinogens    46.066667\n",
      "Stimulants       37.800000\n",
      "Depressants      16.133333\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "train_df = pd.read_csv('data/data_minihackathon_train_engineered.csv')\n",
    "test_df = pd.read_csv('data/data_minihackathon_test_engineered.csv')\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"\\nClass distribution BEFORE balancing:\")\n",
    "print(train_df['drug_category'].value_counts())\n",
    "print(f\"\\nPercentages:\")\n",
    "print(train_df['drug_category'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb4f82f",
   "metadata": {},
   "source": [
    "## Balance Dataset Using Undersampling\n",
    "We'll undersample Hallucinogens and Stimulants to match Depressants frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "72a86ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (1500, 45)\n",
      "Target shape: (1500,)\n",
      "\n",
      "Label encoding mapping:\n",
      "  Depressants: 0\n",
      "  Hallucinogens: 1\n",
      "  Stimulants: 2\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = train_df.drop(['drug_category'], axis=1)\n",
    "y = train_df['drug_category']\n",
    "\n",
    "# Check for ID column and drop it\n",
    "if 'id' in X.columns:\n",
    "    X = X.drop(['id'], axis=1)\n",
    "\n",
    "# Encode labels to numeric values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "class_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(\"\\nLabel encoding mapping:\")\n",
    "for label, encoded in class_mapping.items():\n",
    "    print(f\"  {label}: {encoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b5a66d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Balancing Strategy: Moderate undersampling with factor 1.5\n",
      "Original class counts:\n",
      "drug_category\n",
      "Hallucinogens    691\n",
      "Stimulants       567\n",
      "Depressants      242\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Target count for Hallucinogens: 363 (original 691)\n",
      "Target count for Stimulants: 363 (original 567)\n",
      "Target count for Depressants: 242 (original 242)\n",
      "\n",
      "Balanced dataset shape: (968, 45)\n",
      "\n",
      "Class distribution AFTER balancing:\n",
      "Hallucinogens    363\n",
      "Stimulants       363\n",
      "Depressants      242\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentages:\n",
      "Hallucinogens    37.5\n",
      "Stimulants       37.5\n",
      "Depressants      25.0\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Strategy: Undersample majority classes but keep a bit more data than the smallest class\n",
    "# This keeps Depressants intact while allowing Hallucinogens/Stimulants slightly higher counts\n",
    "target_factor = 1.5  # keep up to 150% of depressant count for other classes\n",
    "\n",
    "# Count samples per class\n",
    "class_counts = y.value_counts()\n",
    "min_class_count = class_counts.min()\n",
    "target_counts = {}\n",
    "\n",
    "print(\"\\nBalancing Strategy: Moderate undersampling with factor\", target_factor)\n",
    "print(f\"Original class counts:\\n{class_counts}\\n\")\n",
    "\n",
    "for cls_name, original_count in class_counts.items():\n",
    "    encoded_label = int(label_encoder.transform([cls_name])[0])\n",
    "    if cls_name == 'Depressants':\n",
    "        target_counts[encoded_label] = original_count\n",
    "    else:\n",
    "        target_counts[encoded_label] = min(int(min_class_count * target_factor), original_count)\n",
    "    print(f\"Target count for {cls_name}: {target_counts[encoded_label]} (original {original_count})\")\n",
    "\n",
    "# Use RandomUnderSampler with custom strategy\n",
    "undersampler = RandomUnderSampler(sampling_strategy=target_counts, random_state=42)\n",
    "\n",
    "# Create moderately balanced dataset\n",
    "X_balanced, y_balanced_encoded = undersampler.fit_resample(X, y_encoded)\n",
    "\n",
    "# Decode back to string labels for display\n",
    "y_balanced = label_encoder.inverse_transform(y_balanced_encoded)\n",
    "balanced_counts = pd.Series(y_balanced).value_counts()\n",
    "balanced_percentages = balanced_counts / balanced_counts.sum() * 100\n",
    "\n",
    "print(f\"\\nBalanced dataset shape: {X_balanced.shape}\")\n",
    "print(\"\\nClass distribution AFTER balancing:\")\n",
    "print(balanced_counts)\n",
    "print(\"\\nPercentages:\")\n",
    "print(balanced_percentages.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3549033",
   "metadata": {},
   "source": [
    "## Train-Test Split for Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a616df41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (774, 45)\n",
      "Validation set: (194, 45)\n",
      "\n",
      "Training set class distribution:\n",
      "Stimulants       290\n",
      "Hallucinogens    290\n",
      "Depressants      194\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation set class distribution:\n",
      "Stimulants       73\n",
      "Hallucinogens    73\n",
      "Depressants      48\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split balanced data for validation\n",
    "X_train, X_val, y_train_encoded, y_val_encoded = train_test_split(\n",
    "    X_balanced, y_balanced_encoded, test_size=0.2, random_state=42, stratify=y_balanced_encoded\n",
    ")\n",
    "\n",
    "# Keep string versions for display\n",
    "y_train = label_encoder.inverse_transform(y_train_encoded)\n",
    "y_val = label_encoder.inverse_transform(y_val_encoded)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(\"\\nTraining set class distribution:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "print(\"\\nValidation set class distribution:\")\n",
    "print(pd.Series(y_val).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57923978",
   "metadata": {},
   "source": [
    "## Train Multiple Models on Balanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0bb1d37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost...\n",
      "XGBoost Validation Accuracy: 0.6495\n",
      "\n",
      "XGBoost Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Depressants       0.30      0.17      0.21        48\n",
      "Hallucinogens       0.80      0.81      0.80        73\n",
      "   Stimulants       0.63      0.81      0.71        73\n",
      "\n",
      "     accuracy                           0.65       194\n",
      "    macro avg       0.58      0.59      0.58       194\n",
      " weighted avg       0.61      0.65      0.62       194\n",
      "\n",
      "XGBoost Validation Accuracy: 0.6495\n",
      "\n",
      "XGBoost Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Depressants       0.30      0.17      0.21        48\n",
      "Hallucinogens       0.80      0.81      0.80        73\n",
      "   Stimulants       0.63      0.81      0.71        73\n",
      "\n",
      "     accuracy                           0.65       194\n",
      "    macro avg       0.58      0.59      0.58       194\n",
      " weighted avg       0.61      0.65      0.62       194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model 1: XGBoost with balanced classes\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=26,\n",
    "    learning_rate=0.01,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    eval_metric='mlogloss',\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "print(\"Training XGBoost...\")\n",
    "xgb_model.fit(X_train, y_train_encoded)\n",
    "xgb_pred_encoded = xgb_model.predict(X_val)\n",
    "xgb_pred = label_encoder.inverse_transform(xgb_pred_encoded)\n",
    "xgb_acc = accuracy_score(y_val, xgb_pred)\n",
    "print(f\"XGBoost Validation Accuracy: {xgb_acc:.4f}\")\n",
    "print(\"\\nXGBoost Classification Report:\")\n",
    "print(classification_report(y_val, xgb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "adddb7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM...\n",
      "LightGBM Validation Accuracy: 0.6289\n",
      "\n",
      "LightGBM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Depressants       0.27      0.17      0.21        48\n",
      "Hallucinogens       0.79      0.78      0.79        73\n",
      "   Stimulants       0.62      0.78      0.69        73\n",
      "\n",
      "     accuracy                           0.63       194\n",
      "    macro avg       0.56      0.58      0.56       194\n",
      " weighted avg       0.60      0.63      0.61       194\n",
      "\n",
      "LightGBM Validation Accuracy: 0.6289\n",
      "\n",
      "LightGBM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Depressants       0.27      0.17      0.21        48\n",
      "Hallucinogens       0.79      0.78      0.79        73\n",
      "   Stimulants       0.62      0.78      0.69        73\n",
      "\n",
      "     accuracy                           0.63       194\n",
      "    macro avg       0.56      0.58      0.56       194\n",
      " weighted avg       0.60      0.63      0.61       194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model 2: LightGBM with balanced classes\n",
    "lgbm_model = LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=26,\n",
    "    learning_rate=0.01,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "print(\"Training LightGBM...\")\n",
    "lgbm_model.fit(X_train, y_train_encoded)\n",
    "lgbm_pred_encoded = lgbm_model.predict(X_val)\n",
    "lgbm_pred = label_encoder.inverse_transform(lgbm_pred_encoded)\n",
    "lgbm_acc = accuracy_score(y_val, lgbm_pred)\n",
    "print(f\"LightGBM Validation Accuracy: {lgbm_acc:.4f}\")\n",
    "print(\"\\nLightGBM Classification Report:\")\n",
    "print(classification_report(y_val, lgbm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5c4dd12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest...\n",
      "Random Forest Validation Accuracy: 0.6495\n",
      "\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Depressants       0.30      0.15      0.20        48\n",
      "Hallucinogens       0.76      0.81      0.78        73\n",
      "   Stimulants       0.65      0.82      0.72        73\n",
      "\n",
      "     accuracy                           0.65       194\n",
      "    macro avg       0.57      0.59      0.57       194\n",
      " weighted avg       0.60      0.65      0.61       194\n",
      "\n",
      "Random Forest Validation Accuracy: 0.6495\n",
      "\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Depressants       0.30      0.15      0.20        48\n",
      "Hallucinogens       0.76      0.81      0.78        73\n",
      "   Stimulants       0.65      0.82      0.72        73\n",
      "\n",
      "     accuracy                           0.65       194\n",
      "    macro avg       0.57      0.59      0.57       194\n",
      " weighted avg       0.60      0.65      0.61       194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model 3: Random Forest with balanced classes\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model.fit(X_train, y_train_encoded)\n",
    "rf_pred_encoded = rf_model.predict(X_val)\n",
    "rf_pred = label_encoder.inverse_transform(rf_pred_encoded)\n",
    "rf_acc = accuracy_score(y_val, rf_pred)\n",
    "print(f\"Random Forest Validation Accuracy: {rf_acc:.4f}\")\n",
    "print(\"\\nRandom Forest Classification Report:\")\n",
    "print(classification_report(y_val, rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ce9d332a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gradient Boosting...\n",
      "Gradient Boosting Validation Accuracy: 0.6443\n",
      "\n",
      "Gradient Boosting Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Depressants       0.23      0.10      0.14        48\n",
      "Hallucinogens       0.78      0.84      0.81        73\n",
      "   Stimulants       0.63      0.81      0.71        73\n",
      "\n",
      "     accuracy                           0.64       194\n",
      "    macro avg       0.55      0.58      0.55       194\n",
      " weighted avg       0.59      0.64      0.61       194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model 4: Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=15,\n",
    "    learning_rate=0.01,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training Gradient Boosting...\")\n",
    "gb_model.fit(X_train, y_train_encoded)\n",
    "gb_pred_encoded = gb_model.predict(X_val)\n",
    "gb_pred = label_encoder.inverse_transform(gb_pred_encoded)\n",
    "gb_acc = accuracy_score(y_val, gb_pred)\n",
    "print(f\"Gradient Boosting Validation Accuracy: {gb_acc:.4f}\")\n",
    "print(\"\\nGradient Boosting Classification Report:\")\n",
    "print(classification_report(y_val, gb_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93f94e3",
   "metadata": {},
   "source": [
    "## Create Voting Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fedbdd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Voting Ensemble...\n",
      "Voting Ensemble Validation Accuracy: 0.6340\n",
      "\n",
      "Voting Ensemble Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Depressants       0.20      0.10      0.14        48\n",
      "Hallucinogens       0.79      0.81      0.80        73\n",
      "   Stimulants       0.63      0.81      0.71        73\n",
      "\n",
      "     accuracy                           0.63       194\n",
      "    macro avg       0.54      0.57      0.55       194\n",
      " weighted avg       0.58      0.63      0.60       194\n",
      "\n",
      "Voting Ensemble Validation Accuracy: 0.6340\n",
      "\n",
      "Voting Ensemble Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Depressants       0.20      0.10      0.14        48\n",
      "Hallucinogens       0.79      0.81      0.80        73\n",
      "   Stimulants       0.63      0.81      0.71        73\n",
      "\n",
      "     accuracy                           0.63       194\n",
      "    macro avg       0.54      0.57      0.55       194\n",
      " weighted avg       0.58      0.63      0.60       194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create voting ensemble with soft voting (uses probabilities)\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', xgb_model),\n",
    "        ('lgbm', lgbm_model),\n",
    "        ('rf', rf_model),\n",
    "        ('gb', gb_model)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "print(\"Training Voting Ensemble...\")\n",
    "voting_clf.fit(X_train, y_train_encoded)\n",
    "voting_pred_encoded = voting_clf.predict(X_val)\n",
    "voting_pred = label_encoder.inverse_transform(voting_pred_encoded)\n",
    "voting_acc = accuracy_score(y_val, voting_pred)\n",
    "print(f\"Voting Ensemble Validation Accuracy: {voting_acc:.4f}\")\n",
    "print(\"\\nVoting Ensemble Classification Report:\")\n",
    "print(classification_report(y_val, voting_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec63729",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ecf1308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "MODEL COMPARISON (sorted by accuracy)\n",
      "==================================================\n",
      "            Model  Accuracy\n",
      "          XGBoost  0.649485\n",
      "    Random Forest  0.649485\n",
      "Gradient Boosting  0.644330\n",
      "  Voting Ensemble  0.634021\n",
      "         LightGBM  0.628866\n",
      "==================================================\n",
      "\n",
      "ðŸ† Best Model: XGBoost with accuracy 0.6495\n"
     ]
    }
   ],
   "source": [
    "# Compare all models\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['XGBoost', 'LightGBM', 'Random Forest', 'Gradient Boosting', 'Voting Ensemble'],\n",
    "    'Accuracy': [xgb_acc, lgbm_acc, rf_acc, gb_acc, voting_acc]\n",
    "})\n",
    "\n",
    "results = results.sort_values('Accuracy', ascending=False)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL COMPARISON (sorted by accuracy)\")\n",
    "print(\"=\"*50)\n",
    "print(results.to_string(index=False))\n",
    "print(\"=\"*50)\n",
    "\n",
    "best_model_name = results.iloc[0]['Model']\n",
    "best_accuracy = results.iloc[0]['Accuracy']\n",
    "print(f\"\\nðŸ† Best Model: {best_model_name} with accuracy {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd625e3",
   "metadata": {},
   "source": [
    "## Cross-Validation on Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4dd25cc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Perform cross-validation on the voting ensemble\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m cv_scores = \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvoting_clf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_balanced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_balanced_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maccuracy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCross-Validation Scores: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcv_scores\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMean CV Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcv_scores.mean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (+/- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcv_scores.std()\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m2\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\Vision\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\Vision\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:677\u001b[39m, in \u001b[36mcross_val_score\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[32m    675\u001b[39m scorer = check_scoring(estimator, scoring=scoring)\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m cv_results = \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[33m\"\u001b[39m\u001b[33mtest_score\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\Vision\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\Vision\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:399\u001b[39m, in \u001b[36mcross_validate\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[32m    398\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m results = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    419\u001b[39m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[32m    421\u001b[39m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[32m    423\u001b[39m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\Vision\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\Vision\\Lib\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\Vision\\Lib\\site-packages\\joblib\\parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\Vision\\Lib\\site-packages\\sklearn\\utils\\parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\Vision\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:859\u001b[39m, in \u001b[36m_fit_and_score\u001b[39m\u001b[34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[39m\n\u001b[32m    857\u001b[39m         estimator.fit(X_train, **fit_params)\n\u001b[32m    858\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m859\u001b[39m         \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    862\u001b[39m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[32m    863\u001b[39m     fit_time = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\Vision\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\Vision\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:405\u001b[39m, in \u001b[36mVotingClassifier.fit\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    402\u001b[39m \u001b[38;5;28mself\u001b[39m.classes_ = \u001b[38;5;28mself\u001b[39m.le_.classes_\n\u001b[32m    403\u001b[39m transformed_y = \u001b[38;5;28mself\u001b[39m.le_.transform(y)\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\Vision\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:99\u001b[39m, in \u001b[36m_BaseVoting.fit\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33msample_weight\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m fit_params:\n\u001b[32m     95\u001b[39m             routed_params[name].fit[\u001b[33m\"\u001b[39m\u001b[33msample_weight\u001b[39m\u001b[33m\"\u001b[39m] = fit_params[\n\u001b[32m     96\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msample_weight\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     97\u001b[39m             ]\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_ = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_single_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mVoting\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclfs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclfs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdrop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    110\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;28mself\u001b[39m.named_estimators_ = Bunch()\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# Uses 'drop' as placeholder for dropped estimators\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\Vision\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\Vision\\Lib\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\Vision\\Lib\\site-packages\\joblib\\parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\Vision\\Lib\\site-packages\\sklearn\\utils\\parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\Vision\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:39\u001b[39m, in \u001b[36m_fit_single_estimator\u001b[39m\u001b[34m(estimator, X, y, fit_params, message_clsname, message)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m         \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\Vision\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\Vision\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:787\u001b[39m, in \u001b[36mBaseGradientBoosting.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, monitor)\u001b[39m\n\u001b[32m    784\u001b[39m     \u001b[38;5;28mself\u001b[39m._resize_state()\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m n_stages = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_stages != \u001b[38;5;28mself\u001b[39m.estimators_.shape[\u001b[32m0\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\Vision\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:883\u001b[39m, in \u001b[36mBaseGradientBoosting._fit_stages\u001b[39m\u001b[34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[39m\n\u001b[32m    876\u001b[39m         initial_loss = factor * \u001b[38;5;28mself\u001b[39m._loss(\n\u001b[32m    877\u001b[39m             y_true=y_oob_masked,\n\u001b[32m    878\u001b[39m             raw_prediction=raw_predictions[~sample_mask],\n\u001b[32m    879\u001b[39m             sample_weight=sample_weight_oob_masked,\n\u001b[32m    880\u001b[39m         )\n\u001b[32m    882\u001b[39m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m883\u001b[39m raw_predictions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[32m    896\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\Vision\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:495\u001b[39m, in \u001b[36mBaseGradientBoosting._fit_stage\u001b[39m\u001b[34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[32m    494\u001b[39m X_for_tree_update = X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m \u001b[43m_update_terminal_regions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_for_tree_update\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mneg_g_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[38;5;66;03m# add tree to ensemble\u001b[39;00m\n\u001b[32m    509\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_[i, k] = tree\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\Vision\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:255\u001b[39m, in \u001b[36m_update_terminal_regions\u001b[39m\u001b[34m(loss, tree, X, y, neg_gradient, raw_prediction, sample_weight, sample_mask, learning_rate, k)\u001b[39m\n\u001b[32m    253\u001b[39m y_ = y.take(indices, axis=\u001b[32m0\u001b[39m)\n\u001b[32m    254\u001b[39m sw = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight[indices]\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m update = \u001b[43mcompute_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_gradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_prediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[38;5;66;03m# TODO: Multiply here by learning rate instead of everywhere else.\u001b[39;00m\n\u001b[32m    258\u001b[39m tree.value[leaf, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m] = update\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\anaconda3\\envs\\Vision\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:211\u001b[39m, in \u001b[36m_update_terminal_regions.<locals>.compute_update\u001b[39m\u001b[34m(y_, indices, neg_gradient, raw_prediction, k)\u001b[39m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_update\u001b[39m(y_, indices, neg_gradient, raw_prediction, k):\n\u001b[32m    210\u001b[39m     \u001b[38;5;66;03m# we take advantage that: y - prob = neg_gradient\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     neg_g = \u001b[43mneg_gradient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m     prob = y_ - neg_g\n\u001b[32m    213\u001b[39m     K = loss.n_classes\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Perform cross-validation on the voting ensemble\n",
    "cv_scores = cross_val_score(\n",
    "    voting_clf, X_balanced, y_balanced_encoded, cv=5, scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(f\"\\nCross-Validation Scores: {cv_scores}\")\n",
    "print(f\"Mean CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2866423",
   "metadata": {},
   "source": [
    "## Check Confusion Matrix for All Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa56f530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix (Voting Ensemble):\n",
      "               Depressants  Hallucinogens  Stimulants\n",
      "Depressants             10             11          27\n",
      "Hallucinogens           11             56           6\n",
      "Stimulants              12              5          56\n",
      "\n",
      "Per-Class Accuracy:\n",
      "Depressants: 0.2083 (10/48)\n",
      "Hallucinogens: 0.7671 (56/73)\n",
      "Stimulants: 0.7671 (56/73)\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix for voting ensemble\n",
    "cm = confusion_matrix(y_val, voting_pred)\n",
    "classes = label_encoder.classes_\n",
    "\n",
    "print(\"\\nConfusion Matrix (Voting Ensemble):\")\n",
    "cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "print(cm_df)\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "for idx, cls in enumerate(classes):\n",
    "    row_sum = cm[idx, :].sum()\n",
    "    class_acc = cm[idx, idx] / row_sum if row_sum else 0.0\n",
    "    print(f\"{cls}: {class_acc:.4f} ({cm[idx, idx]}/{row_sum})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229ae4dc",
   "metadata": {},
   "source": [
    "## Retrain on Full Balanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d693c160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Voting Ensemble on full balanced dataset...\n",
      "âœ“ Training complete!\n",
      "âœ“ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Retrain voting ensemble on full balanced dataset\n",
    "print(\"Retraining Voting Ensemble on full balanced dataset...\")\n",
    "\n",
    "# Recreate models with same parameters\n",
    "final_xgb = XGBClassifier(\n",
    "    n_estimators=500, max_depth=26, learning_rate=0.01, subsample=0.8,\n",
    "    colsample_bytree=0.8, random_state=42, eval_metric='mlogloss', tree_method='hist'\n",
    ")\n",
    "\n",
    "final_lgbm = LGBMClassifier(\n",
    "    n_estimators=500, max_depth=26, learning_rate=0.01, subsample=0.8,\n",
    "    colsample_bytree=0.8, random_state=42, verbose=-1\n",
    ")\n",
    "\n",
    "final_rf = RandomForestClassifier(\n",
    "    n_estimators=500, max_depth=20, min_samples_split=5,\n",
    "    min_samples_leaf=2, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "final_gb = GradientBoostingClassifier(\n",
    "    n_estimators=300, max_depth=15, learning_rate=0.01,\n",
    "    subsample=0.8, random_state=42\n",
    ")\n",
    "\n",
    "# Create final voting ensemble\n",
    "final_voting = VotingClassifier(\n",
    "    estimators=[('xgb', final_xgb), ('lgbm', final_lgbm), ('rf', final_rf), ('gb', final_gb)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Train on full balanced dataset\n",
    "final_voting.fit(X_balanced, y_balanced_encoded)\n",
    "print(\"âœ“ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0fe6eb",
   "metadata": {},
   "source": [
    "## Prepare Test Data and Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "766e7a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape: (377, 45)\n",
      "Expected shape: (377, 45)\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data\n",
    "X_test = test_df.copy()\n",
    "\n",
    "# Drop ID column if present\n",
    "if 'id' in X_test.columns:\n",
    "    X_test = X_test.drop(['id'], axis=1)\n",
    "\n",
    "# Ensure test data has same columns as training data\n",
    "missing_cols = set(X_balanced.columns) - set(X_test.columns)\n",
    "extra_cols = set(X_test.columns) - set(X_balanced.columns)\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"Warning: Test data missing columns: {missing_cols}\")\n",
    "if extra_cols:\n",
    "    print(f\"Warning: Test data has extra columns: {extra_cols}\")\n",
    "    X_test = X_test[X_balanced.columns]\n",
    "\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Expected shape: ({len(test_df)}, {X_balanced.shape[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "680b6760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n",
      "\n",
      "Prediction distribution:\n",
      "Hallucinogens    171\n",
      "Stimulants       154\n",
      "Depressants       52\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentages:\n",
      "Hallucinogens    45.358090\n",
      "Stimulants       40.848806\n",
      "Depressants      13.793103\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Prediction distribution:\n",
      "Hallucinogens    171\n",
      "Stimulants       154\n",
      "Depressants       52\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentages:\n",
      "Hallucinogens    45.358090\n",
      "Stimulants       40.848806\n",
      "Depressants      13.793103\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "print(\"Generating predictions...\")\n",
    "predictions_encoded = final_voting.predict(X_test)\n",
    "predictions = label_encoder.inverse_transform(predictions_encoded)\n",
    "prediction_probs = final_voting.predict_proba(X_test)\n",
    "\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(pd.Series(predictions).value_counts())\n",
    "print(f\"\\nPercentages:\")\n",
    "print(pd.Series(predictions).value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cb421c",
   "metadata": {},
   "source": [
    "## Validate Against Known Depressants IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0468e7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VALIDATION: Checking Confirmed Depressants IDs\n",
      "============================================================\n",
      "ID 513: Predicted=Stimulants, Depressants_prob=0.2909 âŒ MISSED\n",
      "ID 521: Predicted=Hallucinogens, Depressants_prob=0.2699 âŒ MISSED\n",
      "ID 570: Predicted=Stimulants, Depressants_prob=0.3759 âŒ MISSED\n",
      "ID 642: Predicted=Hallucinogens, Depressants_prob=0.3744 âŒ MISSED\n",
      "ID 770: Predicted=Depressants, Depressants_prob=0.4473 âœ“ CAUGHT\n",
      "\n",
      "Success Rate: 1/5 (20.0%)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Check predictions for confirmed Depressants IDs\n",
    "confirmed_depressants = [513, 521, 570, 642, 770]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION: Checking Confirmed Depressants IDs\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get class names (need to decode from numeric to string)\n",
    "class_names = label_encoder.classes_\n",
    "depressants_idx = list(class_names).index('Depressants')\n",
    "\n",
    "for dep_id in confirmed_depressants:\n",
    "    test_idx = dep_id - 501  # Convert submission ID to test index\n",
    "    pred = predictions[test_idx]\n",
    "    prob = prediction_probs[test_idx]\n",
    "    dep_prob = prob[depressants_idx]\n",
    "    \n",
    "    status = \"âœ“ CAUGHT\" if pred == 'Depressants' else \"âŒ MISSED\"\n",
    "    print(f\"ID {dep_id}: Predicted={pred}, Depressants_prob={dep_prob:.4f} {status}\")\n",
    "\n",
    "caught = sum(1 for dep_id in confirmed_depressants if predictions[dep_id - 501] == 'Depressants')\n",
    "print(f\"\\nSuccess Rate: {caught}/{len(confirmed_depressants)} ({100*caught/len(confirmed_depressants):.1f}%)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d391e969",
   "metadata": {},
   "source": [
    "## Create Submission File with IDs Starting from 501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fae0d7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Submission file created: submission_BALANCED_3CLASS_20251117_100513.csv\n",
      "\n",
      "Submission shape: (377, 2)\n",
      "\n",
      "First few rows:\n",
      "    ID  drug_category\n",
      "0  501     Stimulants\n",
      "1  502     Stimulants\n",
      "2  503     Stimulants\n",
      "3  504  Hallucinogens\n",
      "4  505     Stimulants\n",
      "5  506  Hallucinogens\n",
      "6  507    Depressants\n",
      "7  508     Stimulants\n",
      "8  509  Hallucinogens\n",
      "9  510     Stimulants\n",
      "\n",
      "Last few rows:\n",
      "      ID  drug_category\n",
      "367  868     Stimulants\n",
      "368  869    Depressants\n",
      "369  870  Hallucinogens\n",
      "370  871     Stimulants\n",
      "371  872  Hallucinogens\n",
      "372  873  Hallucinogens\n",
      "373  874  Hallucinogens\n",
      "374  875  Hallucinogens\n",
      "375  876  Hallucinogens\n",
      "376  877  Hallucinogens\n",
      "\n",
      "Final prediction distribution:\n",
      "drug_category\n",
      "Hallucinogens    171\n",
      "Stimulants       154\n",
      "Depressants       52\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentages:\n",
      "drug_category\n",
      "Hallucinogens    45.358090\n",
      "Stimulants       40.848806\n",
      "Depressants      13.793103\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'ID': range(501, 501 + len(predictions)),  # IDs from 501 to 877\n",
    "    'drug_category': predictions\n",
    "})\n",
    "\n",
    "# Generate timestamp for filename\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "filename = f'submission_BALANCED_3CLASS_{timestamp}.csv'\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv(filename, index=False)\n",
    "print(f\"\\nâœ“ Submission file created: {filename}\")\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(submission.head(10))\n",
    "print(f\"\\nLast few rows:\")\n",
    "print(submission.tail(10))\n",
    "print(f\"\\nFinal prediction distribution:\")\n",
    "print(submission['drug_category'].value_counts())\n",
    "print(f\"\\nPercentages:\")\n",
    "print(submission['drug_category'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87837c7d",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a5a55599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL SUMMARY\n",
      "============================================================\n",
      "Training Strategy: Balanced dataset (undersampling)\n",
      "Original training samples: 1500\n",
      "Balanced training samples: 968\n",
      "Test samples: 377\n",
      "\n",
      "Model: Voting Ensemble (XGBoost + LightGBM + RF + GB)\n",
      "Validation Accuracy: 0.6340\n",
      "Cross-Validation Accuracy: 0.6157 (+/- 0.0717)\n",
      "\n",
      "Predicted Depressants: 52 (13.8%)\n",
      "Predicted Hallucinogens: 171 (45.4%)\n",
      "Predicted Stimulants: 154 (40.8%)\n",
      "\n",
      "Confirmed Depressants caught: 1/5\n",
      "\n",
      "Submission file: submission_BALANCED_3CLASS_20251117_100513.csv\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training Strategy: Balanced dataset (undersampling)\")\n",
    "print(f\"Original training samples: {len(train_df)}\")\n",
    "print(f\"Balanced training samples: {len(X_balanced)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"\\nModel: Voting Ensemble (XGBoost + LightGBM + RF + GB)\")\n",
    "print(f\"Validation Accuracy: {voting_acc:.4f}\")\n",
    "print(f\"Cross-Validation Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "print(f\"\\nPredicted Depressants: {(predictions == 'Depressants').sum()} ({100*(predictions == 'Depressants').sum()/len(predictions):.1f}%)\")\n",
    "print(f\"Predicted Hallucinogens: {(predictions == 'Hallucinogens').sum()} ({100*(predictions == 'Hallucinogens').sum()/len(predictions):.1f}%)\")\n",
    "print(f\"Predicted Stimulants: {(predictions == 'Stimulants').sum()} ({100*(predictions == 'Stimulants').sum()/len(predictions):.1f}%)\")\n",
    "print(f\"\\nConfirmed Depressants caught: {caught}/{len(confirmed_depressants)}\")\n",
    "print(f\"\\nSubmission file: {filename}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
